# =====================================================
# íšŒê·€ë¶„ì„ ë¡œì§ ëª¨ë“ˆ (ì—…ê·¸ë ˆì´ë“œ ë²„ì „)
# statsmodelsë¥¼ ì‚¬ìš©í•œ ë‹¤ì¤‘íšŒê·€ë¶„ì„ êµ¬í˜„
# SPSS ìŠ¤íƒ€ì¼ ê²°ê³¼ ì¶œë ¥ ì§€ì›
# 
# ì£¼ìš” ê¸°ëŠ¥:
# - 2-way ìƒí˜¸ì‘ìš© íš¨ê³¼ ìë™ ìƒì„±
# - VIF > 10 ë³€ìˆ˜ ìë™ ì œê±° (ë‹¤ì¤‘ê³µì„ ì„± í•´ê²°)
# - Jarque-Bera ì •ê·œì„± ê²€ì •
# - ì´ìƒì¹˜ ë¶„ì„ (í‘œì¤€í™” ì”ì°¨ > 3)
# - ì‚¬ë¶„ìœ„ìˆ˜ (Q1, Q3) í¬í•¨
# - ì œê±°ëœ ë³€ìˆ˜ ìƒì„¸ í‘œì‹œ
# =====================================================

import numpy as np
import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.stattools import durbin_watson
from scipy import stats
from itertools import combinations
from typing import Dict, List, Any, Optional, Tuple
import warnings

warnings.filterwarnings('ignore')


def safe_float(value, default=0.0) -> float:
    """
    NaN, Infë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” float ë³€í™˜ í•¨ìˆ˜
    
    Args:
        value: ë³€í™˜í•  ê°’
        default: ê¸°ë³¸ê°’
    
    Returns:
        ì•ˆì „í•˜ê²Œ ë³€í™˜ëœ float ê°’
    """
    import math
    try:
        val = float(value)
        if math.isnan(val) or math.isinf(val):
            return default
        return val
    except (ValueError, TypeError):
        return default


def calculate_vif(df: pd.DataFrame, independent_vars: List[str]) -> Dict[str, float]:
    """
    VIF (Variance Inflation Factor) ê³„ì‚°
    ë‹¤ì¤‘ê³µì„ ì„± ì§„ë‹¨ì— ì‚¬ìš©
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        independent_vars: ë…ë¦½ë³€ìˆ˜ ëª©ë¡
    
    Returns:
        ë³€ìˆ˜ë³„ VIF ë”•ì…”ë„ˆë¦¬
    """
    vif_data = {}
    
    if len(independent_vars) < 2:
        # ë…ë¦½ë³€ìˆ˜ê°€ 1ê°œë©´ VIF = 1
        for var in independent_vars:
            vif_data[var] = 1.0
        return vif_data
    
    X = df[independent_vars].values
    
    for i, var in enumerate(independent_vars):
        try:
            vif = variance_inflation_factor(X, i)
            vif_data[var] = round(vif, 2) if not np.isinf(vif) else 999.99
        except:
            vif_data[var] = 999.99
    
    return vif_data


def remove_multicollinearity(df: pd.DataFrame, independent_vars: List[str], 
                            threshold: float = 10.0) -> Tuple[List[str], List[str]]:
    """
    VIF > threshold ë³€ìˆ˜ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì œê±°í•˜ì—¬ ë‹¤ì¤‘ê³µì„ ì„± í•´ê²°
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        independent_vars: ë…ë¦½ë³€ìˆ˜ ëª©ë¡
        threshold: VIF ì„ê³„ê°’ (ê¸°ë³¸ 10)
    
    Returns:
        (ë‚¨ì€ ë³€ìˆ˜ ëª©ë¡, ì œê±°ëœ ë³€ìˆ˜ ëª©ë¡)
    """
    remaining_vars = independent_vars.copy()
    removed_vars = []
    
    while len(remaining_vars) > 1:
        vif_dict = calculate_vif(df, remaining_vars)
        max_vif = max(vif_dict.values())
        
        if max_vif <= threshold:
            break
        
        # VIFê°€ ê°€ì¥ ë†’ì€ ë³€ìˆ˜ ì œê±°
        max_var = max(vif_dict, key=vif_dict.get)
        remaining_vars.remove(max_var)
        removed_vars.append(f"{max_var} (VIF={vif_dict[max_var]:.2f})")
        print(f"  [ë‹¤ì¤‘ê³µì„ ì„± ì œê±°] {max_var} (VIF={vif_dict[max_var]})")
    
    return remaining_vars, removed_vars


def calculate_descriptive_stats(df: pd.DataFrame, columns: List[str]) -> List[Dict[str, Any]]:
    """
    ê¸°ìˆ í†µê³„ëŸ‰ ê³„ì‚° (Q1, Q3 í¬í•¨)
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        columns: ë¶„ì„í•  ì»¬ëŸ¼ ëª©ë¡
    
    Returns:
        ê¸°ìˆ í†µê³„ëŸ‰ ë¦¬ìŠ¤íŠ¸
    """
    stats_list = []
    
    for col in columns:
        if col in df.columns:
            col_data = pd.to_numeric(df[col], errors='coerce').dropna()
            
            if len(col_data) > 0:
                stats_list.append({
                    "variable": col,
                    "n": int(len(col_data)),
                    "mean": float(round(col_data.mean(), 4)),
                    "std": float(round(col_data.std(), 4)),
                    "min": float(round(col_data.min(), 4)),
                    "q25": float(round(col_data.quantile(0.25), 4)),  # Q1 ì¶”ê°€
                    "median": float(round(col_data.median(), 4)),
                    "q75": float(round(col_data.quantile(0.75), 4)),  # Q3 ì¶”ê°€
                    "max": float(round(col_data.max(), 4)),
                    "skewness": float(round(col_data.skew(), 4)),
                    "kurtosis": float(round(col_data.kurtosis(), 4))
                })
    
    return stats_list


def calculate_correlation_matrix(df: pd.DataFrame, columns: List[str]) -> List[Dict[str, Any]]:
    """
    ìƒê´€í–‰ë ¬ ê³„ì‚°
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        columns: ë¶„ì„í•  ì»¬ëŸ¼ ëª©ë¡
    
    Returns:
        ìƒê´€í–‰ë ¬ ë¦¬ìŠ¤íŠ¸
    """
    # ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
    numeric_df = df[columns].apply(pd.to_numeric, errors='coerce').dropna()
    
    if len(numeric_df) < 3:
        return []
    
    # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
    corr_matrix = numeric_df.corr()
    
    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
    correlation_list = []
    for i, var1 in enumerate(columns):
        row = {"variable": var1}
        for var2 in columns:
            if var1 in corr_matrix.index and var2 in corr_matrix.columns:
                row[var2] = round(float(corr_matrix.loc[var1, var2]), 4)
            else:
                row[var2] = 0.0
        correlation_list.append(row)
    
    return correlation_list


def run_regression(data: List[Dict], 
                   dependent_var: str, 
                   independent_vars: List[str],
                   method: str = "enter") -> Dict[str, Any]:
    """
    ë‹¤ì¤‘íšŒê·€ë¶„ì„ ì‹¤í–‰ (superbase_link ìŠ¤íƒ€ì¼ ì—…ê·¸ë ˆì´ë“œ)
    
    ì£¼ìš” ê¸°ëŠ¥:
    - 2-way ìƒí˜¸ì‘ìš© íš¨ê³¼ ìë™ ìƒì„±
    - VIF > 10 ë³€ìˆ˜ ìë™ ì œê±°
    - Jarque-Bera ì •ê·œì„± ê²€ì •
    - ì´ìƒì¹˜ ë¶„ì„
    - ì‚¬ë¶„ìœ„ìˆ˜ í¬í•¨
    
    Args:
        data: ë°ì´í„° ëª©ë¡ (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
        dependent_var: ì¢…ì†ë³€ìˆ˜ëª…
        independent_vars: ë…ë¦½ë³€ìˆ˜ ëª©ë¡
        method: "enter" ë˜ëŠ” "stepwise"
    
    Returns:
        íšŒê·€ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(data)
        
        # ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        all_vars = [dependent_var] + independent_vars
        analysis_df = df[all_vars].copy()
        
        # ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
        for col in all_vars:
            analysis_df[col] = pd.to_numeric(analysis_df[col], errors='coerce')
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        analysis_df = analysis_df.dropna()
        initial_count = len(analysis_df)
        
        if initial_count < len(independent_vars) + 2:
            return {"error": "ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìµœì†Œ (ë…ë¦½ë³€ìˆ˜ ìˆ˜ + 2)ê°œì˜ í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤."}
        
        # =====================================================
        # 1ë‹¨ê³„: ë‹¤ì¤‘ê³µì„ ì„± ì œê±° (VIF > 10)
        # =====================================================
        print(f"\n=== {method.upper()} ë°©ì‹ íšŒê·€ë¶„ì„ ===")
        print(f"ì¢…ì†ë³€ìˆ˜: {dependent_var}")
        print(f"ë…ë¦½ë³€ìˆ˜: {independent_vars}")
        
        final_main_vars, removed_by_vif = remove_multicollinearity(
            analysis_df, independent_vars, threshold=10
        )
        
        if len(final_main_vars) == 0:
            return {"error": "ëª¨ë“  ë…ë¦½ë³€ìˆ˜ê°€ ë‹¤ì¤‘ê³µì„ ì„±ìœ¼ë¡œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤."}
        
        # =====================================================
        # 2ë‹¨ê³„: 2-way ìƒí˜¸ì‘ìš© í•­ ìƒì„±
        # =====================================================
        interaction_terms = []
        if len(final_main_vars) >= 2:
            for var1, var2 in combinations(final_main_vars, 2):
                interaction_name = f"{var1}Ã—{var2}"
                analysis_df[interaction_name] = analysis_df[var1] * analysis_df[var2]
                interaction_terms.append(interaction_name)
            print(f"ìƒì„±ëœ ìƒí˜¸ì‘ìš© í•­: {len(interaction_terms)}ê°œ")
        
        # ì£¼íš¨ê³¼ + ìƒí˜¸ì‘ìš© í•­
        all_analysis_vars = final_main_vars + interaction_terms
        
        # =====================================================
        # 3ë‹¨ê³„: Stepwise ë³€ìˆ˜ ì„ íƒ (methodê°€ stepwiseì¸ ê²½ìš°)
        # =====================================================
        removed_by_pvalue = []
        
        if method == "stepwise":
            current_vars = all_analysis_vars.copy()
            p_threshold = 0.05
            
            # Backward Elimination
            while len(current_vars) > 0:
                X = sm.add_constant(analysis_df[current_vars])
                y = analysis_df[dependent_var]
                model = sm.OLS(y, X).fit()
                
                # p-valueê°€ ê°€ì¥ ë†’ì€ ë³€ìˆ˜ ì°¾ê¸° (ìƒìˆ˜í•­ ì œì™¸)
                pvalues = model.pvalues.drop('const')
                max_pvalue = pvalues.max()
                
                if max_pvalue > p_threshold:
                    var_to_remove = pvalues.idxmax()
                    current_vars.remove(var_to_remove)
                    removed_by_pvalue.append(f"{var_to_remove} (p={max_pvalue:.4f})")
                    print(f"  [p-value ì œê±°] {var_to_remove} (p={max_pvalue:.4f})")
                else:
                    break
            
            if len(current_vars) == 0:
                return {"error": "ëª¨ë“  ë…ë¦½ë³€ìˆ˜ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. (p-value > 0.05)"}
            
            all_analysis_vars = current_vars
            # ìµœì¢… ì£¼íš¨ê³¼ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
            final_main_vars = [v for v in all_analysis_vars if v in final_main_vars]
            interaction_terms = [v for v in all_analysis_vars if v in interaction_terms]
        
        # =====================================================
        # 4ë‹¨ê³„: ìµœì¢… íšŒê·€ë¶„ì„ ì‹¤í–‰
        # =====================================================
        X = analysis_df[all_analysis_vars]
        y = analysis_df[dependent_var]
        X_with_const = sm.add_constant(X)
        
        model = sm.OLS(y, X_with_const).fit()
        
        # ì˜ˆì¸¡ê°’ ë° ì”ì°¨
        y_pred = model.predict(X_with_const)
        residuals = y - y_pred
        
        # =====================================================
        # ê¸°ìˆ í†µê³„ëŸ‰ ê³„ì‚° (Q1, Q3 í¬í•¨)
        # =====================================================
        descriptive_vars = [dependent_var] + final_main_vars
        descriptive_stats = calculate_descriptive_stats(analysis_df, descriptive_vars)
        
        # =====================================================
        # ìƒê´€ê´€ê³„ í–‰ë ¬
        # =====================================================
        correlation_matrix = calculate_correlation_matrix(analysis_df, descriptive_vars)
        
        # =====================================================
        # ANOVA í‘œ ê³„ì‚°
        # =====================================================
        n = len(y)
        k = len(all_analysis_vars)
        
        y_mean = y.mean()
        sst = float(np.sum((y - y_mean) ** 2))
        ssr = float(np.sum((y_pred - y_mean) ** 2))
        sse = float(np.sum(residuals ** 2))
        
        df_regression = k
        df_residual = n - k - 1
        df_total = n - 1
        
        msr = ssr / df_regression if df_regression > 0 else 0
        mse = sse / df_residual if df_residual > 0 else 0
        
        anova_table = [
            {
                "source": "íšŒê·€(Regression)",
                "ss": round(safe_float(ssr), 4),
                "df": int(df_regression),
                "ms": round(safe_float(msr), 4),
                "f": round(safe_float(model.fvalue), 4),
                "p_value": round(safe_float(model.f_pvalue), 6)
            },
            {
                "source": "ì”ì°¨(Residual)",
                "ss": round(safe_float(sse), 4),
                "df": int(df_residual),
                "ms": round(safe_float(mse), 4),
                "f": None,
                "p_value": None
            },
            {
                "source": "ì „ì²´(Total)",
                "ss": round(safe_float(sst), 4),
                "df": int(df_total),
                "ms": None,
                "f": None,
                "p_value": None
            }
        ]
        
        # =====================================================
        # ì”ì°¨ ì§„ë‹¨ (Jarque-Bera, ì´ìƒì¹˜ ë¶„ì„ ì¶”ê°€)
        # =====================================================
        standardized_residuals = residuals / np.std(residuals)
        
        # Jarque-Bera ì •ê·œì„± ê²€ì •
        jb_stat, jb_pvalue = stats.jarque_bera(residuals)
        
        # Durbin-Watson ìê¸°ìƒê´€ ê²€ì •
        dw_stat = float(durbin_watson(residuals))
        
        # ì´ìƒì¹˜ ë¶„ì„ (í‘œì¤€í™” ì”ì°¨ ì ˆëŒ“ê°’ > 3)
        outliers_count = int(np.sum(np.abs(standardized_residuals) > 3))
        outliers_percent = round(outliers_count / len(residuals) * 100, 2)
        
        residual_stats = {
            "mean": round(safe_float(residuals.mean()), 6),
            "std": round(safe_float(residuals.std()), 4),
            "min": round(safe_float(residuals.min()), 4),
            "max": round(safe_float(residuals.max()), 4),
            "skewness": round(safe_float(residuals.skew()), 4),
            "kurtosis": round(safe_float(residuals.kurtosis()), 4),
            "durbin_watson": round(dw_stat, 4),
            "jarque_bera_stat": round(safe_float(jb_stat), 4),
            "jarque_bera_pvalue": round(safe_float(jb_pvalue), 6),
            "outliers_count": outliers_count,
            "outliers_percent": outliers_percent
        }
        
        # =====================================================
        # VIF ê³„ì‚° (ì£¼íš¨ê³¼ë§Œ)
        # =====================================================
        vif_dict = calculate_vif(analysis_df, final_main_vars) if final_main_vars else {}
        
        # =====================================================
        # ê³„ìˆ˜ ì •ë³´ (í‘œì¤€í™” ê³„ìˆ˜ í¬í•¨)
        # =====================================================
        std_y = y.std()
        coefficients = []
        
        # ìƒìˆ˜í•­
        coefficients.append({
            "variable": "ìƒìˆ˜í•­",
            "b": round(safe_float(model.params['const']), 6),
            "std_error": round(safe_float(model.bse['const']), 6),
            "beta": None,  # ìƒìˆ˜í•­ì€ í‘œì¤€í™” ê³„ìˆ˜ ì—†ìŒ
            "t_statistic": round(safe_float(model.tvalues['const']), 4),
            "p_value": round(safe_float(model.pvalues['const']), 6),
            "tolerance": None,
            "vif": None,
            "var_type": "constant"
        })
        
        # ì£¼íš¨ê³¼
        for var in final_main_vars:
            std_x = analysis_df[var].std()
            beta = model.params[var] * (std_x / std_y) if std_y != 0 else 0
            
            vif_value = vif_dict.get(var)
            tolerance = round(1.0 / vif_value, 4) if vif_value and vif_value > 0 else None
            
            coefficients.append({
                "variable": var,
                "b": round(safe_float(model.params[var]), 6),
                "std_error": round(safe_float(model.bse[var]), 6),
                "beta": round(safe_float(beta), 6),
                "t_statistic": round(safe_float(model.tvalues[var]), 4),
                "p_value": round(safe_float(model.pvalues[var]), 6),
                "tolerance": tolerance,
                "vif": round(safe_float(vif_value), 2) if vif_value else None,
                "var_type": "main"
            })
        
        # ìƒí˜¸ì‘ìš© í•­
        for var in interaction_terms:
            std_x = analysis_df[var].std()
            beta = model.params[var] * (std_x / std_y) if std_y != 0 else 0
            
            coefficients.append({
                "variable": var,
                "b": round(safe_float(model.params[var]), 6),
                "std_error": round(safe_float(model.bse[var]), 6),
                "beta": round(safe_float(beta), 6),
                "t_statistic": round(safe_float(model.tvalues[var]), 4),
                "p_value": round(safe_float(model.pvalues[var]), 6),
                "tolerance": None,  # ìƒí˜¸ì‘ìš© í•­ì€ VIF ê³„ì‚° ì œì™¸
                "vif": None,
                "var_type": "interaction"
            })
        
        # =====================================================
        # íšŒê·€ì‹ ìƒì„±
        # =====================================================
        equation_parts = [f"{round(model.params['const'], 4)} Ã— ìƒìˆ˜í•­"]
        for var in all_analysis_vars:
            coef = model.params[var]
            sign = "+" if coef >= 0 else "-"
            equation_parts.append(f"{sign} {abs(round(coef, 4))} Ã— {var}")
        
        regression_equation = f"Y = " + " ".join(equation_parts)
        
        # =====================================================
        # ì‹¤ì¸¡ì¹˜/ì˜ˆì¸¡ì¹˜ (ì²« 50ê°œ)
        # =====================================================
        actual_vs_predicted = [
            {
                "index": i + 1,
                "actual": round(safe_float(actual), 4),
                "predicted": round(safe_float(pred), 4),
                "residual": round(safe_float(res), 4)
            }
            for i, (actual, pred, res) in enumerate(zip(y.values[:50], y_pred.values[:50], residuals.values[:50]))
        ]
        
        # =====================================================
        # ì‚°ì ë„ ë°ì´í„° (ì£¼ìš” ë³€ìˆ˜ë³„ X-Y ê´€ê³„)
        # =====================================================
        scatter_data = {}
        for var in final_main_vars:
            scatter_data[var] = [
                {
                    "x": round(safe_float(x_val), 4),
                    "y": round(safe_float(y_val), 4)
                }
                for x_val, y_val in zip(analysis_df[var].values, y.values)
            ]
        
        # =====================================================
        # ì œê±°ëœ ë³€ìˆ˜ ëª©ë¡
        # =====================================================
        all_removed_vars = removed_by_vif + removed_by_pvalue
        
        # =====================================================
        # ê²°ê³¼ í•´ì„ ìƒì„±
        # =====================================================
        interpretation = generate_interpretation(
            model, coefficients, all_removed_vars, vif_dict,
            final_main_vars, interaction_terms, residual_stats
        )
        
        # =====================================================
        # ê²°ê³¼ ë°˜í™˜
        # =====================================================
        r_multiple = float(np.sqrt(model.rsquared))
        
        result = {
            "success": True,
            "method": method,
            "n_observations": int(n),
            "dependent_variable": dependent_var,
            "independent_variables": all_analysis_vars,
            "final_main_vars": final_main_vars,
            "interaction_terms": interaction_terms,
            "removed_vars": all_removed_vars,
            
            # íšŒê·€ì‹
            "regression_equation": regression_equation,
            
            # ëª¨ë¸ ìš”ì•½
            "model_summary": {
                "r": round(r_multiple, 4),
                "r_squared": round(safe_float(model.rsquared), 4),
                "adj_r_squared": round(safe_float(model.rsquared_adj), 4),
                "std_error_estimate": round(np.sqrt(mse), 4),
                "durbin_watson": round(dw_stat, 4),
                "f_statistic": round(safe_float(model.fvalue), 4),
                "f_pvalue": round(safe_float(model.f_pvalue), 6),
                "aic": round(safe_float(model.aic), 4),
                "bic": round(safe_float(model.bic), 4),
                "log_likelihood": round(safe_float(model.llf), 4)
            },
            
            # ANOVA í…Œì´ë¸”
            "anova_table": anova_table,
            
            # ê³„ìˆ˜
            "coefficients": coefficients,
            
            # ê¸°ìˆ í†µê³„ëŸ‰ (Q1, Q3 í¬í•¨)
            "descriptive_stats": descriptive_stats,
            
            # ìƒê´€í–‰ë ¬
            "correlation_matrix": correlation_matrix,
            
            # ì”ì°¨ ì§„ë‹¨ (Jarque-Bera, ì´ìƒì¹˜ í¬í•¨)
            "residual_stats": residual_stats,
            
            # ì‹¤ì¸¡ì¹˜/ì˜ˆì¸¡ì¹˜
            "actual_vs_predicted": actual_vs_predicted,
            
            # ì‚°ì ë„ ë°ì´í„°
            "scatter_data": scatter_data,
            
            # í•´ì„
            "interpretation": interpretation
        }
        
        return result
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}


def generate_interpretation(model, coefficients: List[Dict], removed_vars: List[str],
                           vif_dict: Dict, final_main_vars: List[str], 
                           interaction_terms: List[str], residual_stats: Dict) -> str:
    """
    íšŒê·€ë¶„ì„ ê²°ê³¼ í•´ì„ ìƒì„±
    
    Args:
        model: íšŒê·€ ëª¨ë¸
        coefficients: ê³„ìˆ˜ ì •ë³´
        removed_vars: ì œê±°ëœ ë³€ìˆ˜
        vif_dict: VIF ë”•ì…”ë„ˆë¦¬
        final_main_vars: ìµœì¢… ì£¼íš¨ê³¼ ë³€ìˆ˜
        interaction_terms: ìƒí˜¸ì‘ìš© í•­
        residual_stats: ì”ì°¨ í†µê³„
    
    Returns:
        í•´ì„ ë¬¸ìì—´
    """
    interpretation = []
    
    adj_r2 = model.rsquared_adj
    
    # 1. ëª¨ë¸ ì í•©ë„
    if adj_r2 >= 0.7:
        interpretation.append(f"ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤ (Adjusted RÂ² = {adj_r2:.3f}).")
    elif adj_r2 >= 0.5:
        interpretation.append(f"ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤ (Adjusted RÂ² = {adj_r2:.3f}).")
    else:
        interpretation.append(f"ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì´ ë‚®ìŠµë‹ˆë‹¤ (Adjusted RÂ² = {adj_r2:.3f}). ì¶”ê°€ ë³€ìˆ˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # 2. ìœ ì˜í•œ ì£¼íš¨ê³¼
    significant_main = [c for c in coefficients 
                       if c.get('var_type') == 'main' and c['p_value'] < 0.05]
    if significant_main:
        var_names = ', '.join([v['variable'] for v in significant_main])
        interpretation.append(f"ìœ ì˜í•œ ì£¼íš¨ê³¼(main effect): {var_names}.")
    
    # 3. ìœ ì˜í•œ ìƒí˜¸ì‘ìš© íš¨ê³¼
    significant_interaction = [c for c in coefficients 
                              if c.get('var_type') == 'interaction' and c['p_value'] < 0.05]
    if significant_interaction:
        int_names = ', '.join([v['variable'] for v in significant_interaction])
        interpretation.append(f"ìœ ì˜í•œ ìƒí˜¸ì‘ìš© (interaction effect): {int_names}. ë³€ìˆ˜ë“¤ ì¡°ì ˆíš¨ê³¼ê°€ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")
    
    # 4. ì œê±°ëœ ë³€ìˆ˜
    if removed_vars:
        removed = ', '.join(removed_vars)
        interpretation.append(f"ì œê±°ëœ ë³€ìˆ˜: {removed}.")
    
    # 5. ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬
    if vif_dict and len(vif_dict) > 0:
        max_vif = max(vif_dict.values())
        if max_vif < 5:
            interpretation.append("ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë“  VIF < 5).")
        elif max_vif < 10:
            interpretation.append("ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆìœ¼ë‚˜ í—ˆìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤ (VIF < 10).")
    
    # 6. ì”ì°¨ ì •ê·œì„± ê²€ì •
    jb_pvalue = residual_stats.get('jarque_bera_pvalue', 1.0)
    if jb_pvalue > 0.05:
        interpretation.append(f"ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦…ë‹ˆë‹¤ (Jarque-Bera p = {jb_pvalue:.4f}).")
    else:
        interpretation.append(f"âš ï¸ ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (Jarque-Bera p = {jb_pvalue:.4f}).")
    
    # 7. ìê¸°ìƒê´€ ê²€ì •
    dw = residual_stats.get('durbin_watson', 2.0)
    if 1.5 <= dw <= 2.5:
        interpretation.append(f"ìê¸°ìƒê´€ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤ (Durbin-Watson = {dw:.4f}).")
    else:
        interpretation.append(f"âš ï¸ ìê¸°ìƒê´€ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (Durbin-Watson = {dw:.4f}).")
    
    # 8. ì´ìƒì¹˜
    outliers = residual_stats.get('outliers_count', 0)
    if outliers > 0:
        interpretation.append(f"âš ï¸ ì´ìƒì¹˜ {outliers}ê°œ ë°œê²¬ ({residual_stats.get('outliers_percent', 0):.2f}%).")
    
    # 9. F-í†µê³„ëŸ‰
    if model.f_pvalue < 0.001:
        interpretation.append("ëª¨ë¸ì´ í†µê³„ì ìœ¼ë¡œ ë§¤ìš° ìœ ì˜í•©ë‹ˆë‹¤ (p < 0.001).")
    elif model.f_pvalue < 0.05:
        interpretation.append("ëª¨ë¸ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•©ë‹ˆë‹¤ (p < 0.05).")
    
    return ' '.join(interpretation)


def calculate_interaction_effects(data: List[Dict],
                                   dependent_var: str,
                                   independent_vars: List[str]) -> Dict[str, Any]:
    """
    ìƒí˜¸ì‘ìš© íš¨ê³¼ ê³„ì‚° (ë³„ë„ ë¶„ì„ìš©)
    
    Args:
        data: ë°ì´í„° ëª©ë¡
        dependent_var: ì¢…ì†ë³€ìˆ˜
        independent_vars: ë…ë¦½ë³€ìˆ˜ ëª©ë¡
    
    Returns:
        ìƒí˜¸ì‘ìš© íš¨ê³¼ ê²°ê³¼
    """
    try:
        df = pd.DataFrame(data)
        
        # ìˆ«ìí˜• ë³€í™˜
        all_vars = [dependent_var] + independent_vars
        for col in all_vars:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        df = df.dropna()
        
        if len(independent_vars) < 2:
            return {"interactions": []}
        
        interactions = []
        
        # 2-way ìƒí˜¸ì‘ìš©ë§Œ ê³„ì‚°
        for i in range(len(independent_vars)):
            for j in range(i + 1, len(independent_vars)):
                var1 = independent_vars[i]
                var2 = independent_vars[j]
                
                # ìƒí˜¸ì‘ìš© í•­ ìƒì„±
                interaction_name = f"{var1}Ã—{var2}"
                df[interaction_name] = df[var1] * df[var2]
                
                # ìƒí˜¸ì‘ìš© ëª¨ë¸
                X_interaction = sm.add_constant(df[[var1, var2, interaction_name]])
                y = df[dependent_var]
                
                try:
                    model = sm.OLS(y, X_interaction).fit()
                    
                    interactions.append({
                        "variables": [var1, var2],
                        "interaction_term": interaction_name,
                        "coefficient": float(round(model.params[interaction_name], 4)),
                        "t_statistic": float(round(model.tvalues[interaction_name], 4)),
                        "p_value": float(round(model.pvalues[interaction_name], 4)),
                        "significant": model.pvalues[interaction_name] < 0.05
                    })
                except:
                    pass
        
        return {"interactions": interactions}
        
    except Exception as e:
        return {"error": str(e)}


# =====================================================
# Lasso íšŒê·€ë¶„ì„ (L1 ì •ê·œí™”)
# sklearnì˜ LassoCVë¥¼ ì‚¬ìš©í•œ ìë™ ë³€ìˆ˜ ì„ íƒ
# =====================================================

def run_lasso_regression(data: List[Dict], 
                         dependent_var: str, 
                         independent_vars: List[str],
                         correlation_threshold: float = 0.1) -> Dict[str, Any]:
    """
    Lasso íšŒê·€ë¶„ì„ ì‹¤í–‰ (L1 ì •ê·œí™”)
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ê²°ì¸¡ì¹˜ í‰ê· ê°’ ëŒ€ì²´
    - ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ë³€ìˆ˜ í•„í„°ë§
    - LassoCVë¡œ ìµœì  alpha ìë™ íƒìƒ‰
    - coefficientê°€ 0ì´ ì•„ë‹Œ ë³€ìˆ˜ë§Œ ì„ íƒ
    - MAE, RMSE ì •í™•ë„ ì§€í‘œ
    
    Args:
        data: ë°ì´í„° ëª©ë¡ (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
        dependent_var: ì¢…ì†ë³€ìˆ˜ëª…
        independent_vars: ë…ë¦½ë³€ìˆ˜ ëª©ë¡
        correlation_threshold: ìƒê´€ê³„ìˆ˜ í•„í„°ë§ ì„ê³„ê°’ (ê¸°ë³¸ 0.1)
    
    Returns:
        Lasso íšŒê·€ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    from sklearn.linear_model import LassoCV
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import mean_absolute_error, mean_squared_error
    
    try:
        print(f"\n=== LASSO íšŒê·€ë¶„ì„ ===")
        print(f"ì¢…ì†ë³€ìˆ˜: {dependent_var}")
        print(f"ë…ë¦½ë³€ìˆ˜ ê°œìˆ˜: {len(independent_vars)}")
        
        # =====================================================
        # ë³€ìˆ˜ê°€ ë§ìœ¼ë©´ ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’ ë™ì  ì¡°ì •
        # =====================================================
        MAX_VARS_FOR_LASSO = 40  # ì„±ëŠ¥ì„ ìœ„í•œ ìµœëŒ€ ë³€ìˆ˜ ìˆ˜
        
        print(f"ğŸ“Š ìƒê´€ê³„ìˆ˜ í•„í„°ë§ ì„ê³„ê°’: |r| >= {correlation_threshold}")
        
        if len(independent_vars) > MAX_VARS_FOR_LASSO:
            # ë³€ìˆ˜ê°€ ë§ìœ¼ë©´ ë” ì—„ê²©í•œ í•„í„°ë§ ì ìš©
            correlation_threshold = max(0.2, correlation_threshold)
            print(f"âš ï¸ ë³€ìˆ˜ê°€ {len(independent_vars)}ê°œë¡œ ë§ìŒ â†’ ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’ {correlation_threshold}ë¡œ ì¡°ì •")
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(data)
        
        # ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        all_vars = [dependent_var] + independent_vars
        analysis_df = df[all_vars].copy()
        
        # ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
        for col in all_vars:
            analysis_df[col] = pd.to_numeric(analysis_df[col], errors='coerce')
        
        # =====================================================
        # 1ë‹¨ê³„: ê²°ì¸¡ì¹˜ í‰ê· ê°’ ëŒ€ì²´ (Lasso ì „ìš©)
        # =====================================================
        missing_info = {}
        for col in all_vars:
            missing_count = analysis_df[col].isna().sum()
            if missing_count > 0:
                col_mean = analysis_df[col].mean()
                analysis_df[col] = analysis_df[col].fillna(col_mean)
                missing_info[col] = {
                    "missing_count": int(missing_count),
                    "filled_with_mean": round(float(col_mean), 4)
                }
                print(f"  [ê²°ì¸¡ì¹˜ ì²˜ë¦¬] {col}: {missing_count}ê°œ â†’ í‰ê· ê°’({col_mean:.4f})ìœ¼ë¡œ ëŒ€ì²´")
        
        initial_count = len(analysis_df)
        
        # LassoëŠ” p > n ìƒí™©ì—ì„œë„ ì‘ë™ ê°€ëŠ¥í•˜ë¯€ë¡œ ì œì•½ ì¡°ê±´ ì™„í™”
        # ë‹¨, êµì°¨ê²€ì¦(CV=5)ì„ ìœ„í•´ ìµœì†Œ 5ê°œ ì´ìƒì˜ ë°ì´í„°ëŠ” í•„ìš”
        if initial_count < 5:
            return {"error": "ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ìµœì†Œ 5ê°œ ì´ìƒì˜ í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤."}
        
        # =====================================================
        # 2ë‹¨ê³„: ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ë³€ìˆ˜ í•„í„°ë§
        # =====================================================
        y = analysis_df[dependent_var]
        correlations_with_y = {}
        filtered_vars = []
        removed_by_correlation = []
        
        for var in independent_vars:
            try:
                corr = analysis_df[var].corr(y)
                # NaN ì²´í¬
                if pd.isna(corr):
                    corr = 0.0
                    print(f"  [ê²½ê³ ] {var}: ìƒê´€ê³„ìˆ˜ê°€ NaN â†’ 0ìœ¼ë¡œ ëŒ€ì²´")
                correlations_with_y[var] = round(float(corr), 4)
                
                if abs(corr) >= correlation_threshold:
                    filtered_vars.append(var)
                else:
                    removed_by_correlation.append(f"{var} (r={corr:.4f})")
                    print(f"  [ìƒê´€ê³„ìˆ˜ í•„í„°ë§] {var} ì œê±° (r={corr:.4f} < {correlation_threshold})")
            except Exception as corr_err:
                print(f"  [ì˜¤ë¥˜] {var} ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {corr_err}")
                correlations_with_y[var] = 0.0
        
        print(f"ìƒê´€ê³„ìˆ˜ í•„í„°ë§ í›„ ë‚¨ì€ ë³€ìˆ˜: {len(filtered_vars)}ê°œ")
        
        if len(filtered_vars) == 0:
            return {"error": f"ëª¨ë“  ë…ë¦½ë³€ìˆ˜ê°€ ìƒê´€ê³„ìˆ˜ í•„í„°ë§ìœ¼ë¡œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤ (ì„ê³„ê°’: {correlation_threshold})."}
        
        # ë³€ìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ìƒê´€ê³„ìˆ˜ ì ˆëŒ€ê°’ ê¸°ì¤€ ìƒìœ„ ë³€ìˆ˜ë§Œ ì„ íƒ
        if len(filtered_vars) > MAX_VARS_FOR_LASSO:
            # ìƒê´€ê³„ìˆ˜ ì ˆëŒ€ê°’ ê¸°ì¤€ ì •ë ¬
            sorted_vars = sorted(filtered_vars, key=lambda v: abs(correlations_with_y.get(v, 0)), reverse=True)
            excluded_vars = sorted_vars[MAX_VARS_FOR_LASSO:]
            filtered_vars = sorted_vars[:MAX_VARS_FOR_LASSO]
            
            for v in excluded_vars:
                removed_by_correlation.append(f"{v} (r={correlations_with_y.get(v, 0):.4f}, ìƒìœ„ {MAX_VARS_FOR_LASSO}ê°œ ì´ˆê³¼)")
            
            print(f"âš ï¸ ì„±ëŠ¥ ìµœì í™”: ìƒìœ„ {MAX_VARS_FOR_LASSO}ê°œ ë³€ìˆ˜ë§Œ ì„ íƒ (ì œì™¸: {len(excluded_vars)}ê°œ)")
        
        # =====================================================
        # 3ë‹¨ê³„: ë°ì´í„° í‘œì¤€í™” (ìƒìˆ˜ ë³€ìˆ˜ ì œê±°)
        # =====================================================
        # ë¶„ì‚°ì´ 0ì¸ (ìƒìˆ˜) ë³€ìˆ˜ ì œê±°
        valid_vars = []
        for var in filtered_vars:
            var_std = analysis_df[var].std()
            if var_std > 1e-10:
                valid_vars.append(var)
            else:
                print(f"  [ìƒìˆ˜ ë³€ìˆ˜ ì œê±°] {var}: í‘œì¤€í¸ì°¨=0")
        
        if len(valid_vars) == 0:
            return {"error": "ìœ íš¨í•œ ë…ë¦½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë“  ë³€ìˆ˜ì˜ ë¶„ì‚°ì´ 0)."}
        
        filtered_vars = valid_vars  # ê°±ì‹ 
        
        X = analysis_df[filtered_vars].values
        y_values = y.values
        
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        X_scaled = scaler_X.fit_transform(X)
        y_scaled = scaler_y.fit_transform(y_values.reshape(-1, 1)).ravel()
        
        # =====================================================
        # 4ë‹¨ê³„: LassoCVë¡œ ìµœì  alpha íƒìƒ‰ ë° ëª¨ë¸ í•™ìŠµ
        # =====================================================
        # alpha í›„ë³´ê°’ ì„¤ì • (0.001 ~ 10)
        alphas = np.logspace(-3, 1, 100)
        
        # CV fold ìˆ˜ë¥¼ ë°ì´í„° í¬ê¸°ì— ë§ê²Œ ë™ì  ì¡°ì • (ìµœì†Œ 2, ìµœëŒ€ 5)
        n_samples = len(X_scaled)
        cv_folds = min(5, max(2, n_samples // 3))
        print(f"CV folds: {cv_folds} (ìƒ˜í”Œ ìˆ˜: {n_samples})")
        
        lasso_cv = LassoCV(
            alphas=alphas,
            cv=cv_folds,
            max_iter=10000,
            random_state=42,
            n_jobs=-1  # ë³‘ë ¬ ì²˜ë¦¬
        )
        
        lasso_cv.fit(X_scaled, y_scaled)
        
        optimal_alpha = float(lasso_cv.alpha_)
        print(f"ìµœì  alpha: {optimal_alpha:.6f}")
        
        # =====================================================
        # 5ë‹¨ê³„: coefficientê°€ 0ì´ ì•„ë‹Œ ë³€ìˆ˜ ì„ íƒ
        # =====================================================
        lasso_coefficients = lasso_cv.coef_
        selected_vars = []
        selected_coefficients = []
        zero_vars = []
        
        for i, var in enumerate(filtered_vars):
            coef = lasso_coefficients[i]
            if abs(coef) > 1e-10:  # 0ì´ ì•„ë‹Œ ê²½ìš°
                selected_vars.append(var)
                selected_coefficients.append({
                    "variable": var,
                    "coefficient_scaled": round(float(coef), 6),
                    "correlation_with_y": correlations_with_y[var]
                })
                print(f"  [ì„ íƒ] {var}: coef={coef:.6f}")
            else:
                zero_vars.append(var)
                print(f"  [ì œì™¸] {var}: coef=0")
        
        print(f"Lasso ì„ íƒ ë³€ìˆ˜: {len(selected_vars)}ê°œ")
        
        if len(selected_vars) == 0:
            return {"error": "Lassoê°€ ëª¨ë“  ë³€ìˆ˜ë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤. alpha ê°’ì´ ë„ˆë¬´ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."}
        
        # =====================================================
        # 6ë‹¨ê³„: ì˜ˆì¸¡ ë° ì •í™•ë„ ì§€í‘œ ê³„ì‚° (í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• )
        # =====================================================
        from sklearn.model_selection import cross_val_predict, cross_val_score
        
        # êµì°¨ê²€ì¦ìœ¼ë¡œ ì˜ˆì¸¡ê°’ ìƒì„± (ê° ìƒ˜í”Œì€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì¼ ë•Œì˜ ì˜ˆì¸¡ê°’)
        y_pred_scaled = cross_val_predict(lasso_cv, X_scaled, y_scaled, cv=cv_folds)
        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
        
        # MAE, RMSE ê³„ì‚° (í…ŒìŠ¤íŠ¸ ê¸°ì¤€)
        mae = float(mean_absolute_error(y_values, y_pred))
        rmse = float(np.sqrt(mean_squared_error(y_values, y_pred)))
        
        # RÂ² ê³„ì‚° (êµì°¨ê²€ì¦ ê¸°ë°˜ - ì§„ì§œ ì˜ˆì¸¡ ì„±ëŠ¥)
        ss_res = np.sum((y_values - y_pred) ** 2)
        ss_tot = np.sum((y_values - np.mean(y_values)) ** 2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
        
        # í›ˆë ¨ RÂ² (ì°¸ê³ ìš©) - ê³¼ì í•© í™•ì¸ìš©
        y_pred_train = lasso_cv.predict(X_scaled)
        y_pred_train_orig = scaler_y.inverse_transform(y_pred_train.reshape(-1, 1)).ravel()
        ss_res_train = np.sum((y_values - y_pred_train_orig) ** 2)
        r_squared_train = 1 - (ss_res_train / ss_tot) if ss_tot != 0 else 0
        
        # Cross-validation RÂ² ì ìˆ˜
        cv_scores = cross_val_score(lasso_cv, X_scaled, y_scaled, cv=cv_folds, scoring='r2')
        cv_mean = float(np.mean(cv_scores))
        cv_std = float(np.std(cv_scores))
        
        print(f"í›ˆë ¨ RÂ²: {r_squared_train:.4f}, CV RÂ²: {cv_mean:.4f} (Â±{cv_std:.4f})")
        
        # ê³¼ì í•© ê²½ê³ 
        if r_squared_train > 0.95 and cv_mean < 0.5:
            print(f"âš ï¸ ê³¼ì í•© ê²½ê³ : í›ˆë ¨ RÂ²={r_squared_train:.2f}, CV RÂ²={cv_mean:.2f}")
        
        print(f"MAE: {mae:.4f}, RMSE: {rmse:.4f}, RÂ²: {r_squared:.4f}")
        
        # =====================================================
        # 7ë‹¨ê³„: ì›ë˜ ìŠ¤ì¼€ì¼ì˜ ê³„ìˆ˜ ê³„ì‚° (í•´ì„ìš©)
        # =====================================================
        # í‘œì¤€í™”ëœ ê³„ìˆ˜ë¥¼ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ (ì•ˆì „ ì²˜ë¦¬)
        original_scale_coefficients = []
        
        for i, var in enumerate(filtered_vars):
            scaled_coef = lasso_coefficients[i]
            if abs(scaled_coef) > 1e-10:
                try:
                    # ì›ë˜ ìŠ¤ì¼€ì¼ ê³„ìˆ˜: Î²_original = Î²_scaled * (Ïƒ_y / Ïƒ_x)
                    if scaler_X.scale_[i] > 1e-10:
                        original_coef = scaled_coef * (scaler_y.scale_[0] / scaler_X.scale_[i])
                    else:
                        original_coef = scaled_coef  # fallback
                    original_scale_coefficients.append({
                        "variable": var,
                        "coefficient": round(safe_float(original_coef), 6),
                        "coefficient_scaled": round(safe_float(scaled_coef), 6),
                        "correlation_with_y": correlations_with_y.get(var, 0.0)
                    })
                except Exception as coef_err:
                    print(f"  [ê³„ìˆ˜ ë³€í™˜ ì˜¤ë¥˜] {var}: {coef_err}")
                    original_scale_coefficients.append({
                        "variable": var,
                        "coefficient": round(safe_float(scaled_coef), 6),
                        "coefficient_scaled": round(safe_float(scaled_coef), 6),
                        "correlation_with_y": correlations_with_y.get(var, 0.0)
                    })
        
        # Intercept ê³„ì‚° (ì•ˆì „ ì²˜ë¦¬)
        try:
            intercept_scaled = float(lasso_cv.intercept_)
            intercept = scaler_y.mean_[0] + intercept_scaled * scaler_y.scale_[0]
            for i, var in enumerate(filtered_vars):
                if abs(lasso_coefficients[i]) > 1e-10 and scaler_X.scale_[i] > 1e-10:
                    intercept -= (lasso_coefficients[i] * scaler_y.scale_[0] / scaler_X.scale_[i]) * scaler_X.mean_[i]
            intercept = safe_float(intercept)
        except Exception as intercept_err:
            print(f"  [Intercept ê³„ì‚° ì˜¤ë¥˜] {intercept_err}")
            intercept = safe_float(np.mean(y_values))
        
        # =====================================================
        # 8ë‹¨ê³„: íšŒê·€ì‹ ìƒì„±
        # =====================================================
        equation_parts = [f"{round(intercept, 4)}"]
        for coef_info in original_scale_coefficients:
            coef = coef_info['coefficient']
            var = coef_info['variable']
            sign = "+" if coef >= 0 else "-"
            equation_parts.append(f"{sign} {abs(round(coef, 4))} Ã— {var}")
        
        regression_equation = f"Y = " + " ".join(equation_parts)
        
        # =====================================================
        # 9ë‹¨ê³„: ê¸°ìˆ í†µê³„ëŸ‰ ë° ìƒê´€í–‰ë ¬
        # =====================================================
        descriptive_vars = [dependent_var] + selected_vars
        descriptive_stats = calculate_descriptive_stats(analysis_df, descriptive_vars)
        correlation_matrix = calculate_correlation_matrix(analysis_df, descriptive_vars)
        
        # =====================================================
        # 10ë‹¨ê³„: ì‹¤ì¸¡ì¹˜/ì˜ˆì¸¡ì¹˜ (ì²« 50ê°œ)
        # =====================================================
        residuals = y_values - y_pred
        actual_vs_predicted = [
            {
                "index": i + 1,
                "actual": round(safe_float(actual), 4),
                "predicted": round(safe_float(pred), 4),
                "residual": round(safe_float(res), 4)
            }
            for i, (actual, pred, res) in enumerate(zip(y_values[:50], y_pred[:50], residuals[:50]))
        ]
        
        # =====================================================
        # 11ë‹¨ê³„: ì‚°ì ë„ ë°ì´í„° (ìƒìœ„ 6ê°œ ë³€ìˆ˜ë§Œ - ì„±ëŠ¥ ìµœì í™”)
        # =====================================================
        scatter_data = {}
        # ê³„ìˆ˜ ì ˆëŒ€ê°’ ê¸°ì¤€ ìƒìœ„ 6ê°œ ë³€ìˆ˜ë§Œ ì‚°ì ë„ ìƒì„±
        top_vars = sorted(original_scale_coefficients, key=lambda x: abs(x['coefficient']), reverse=True)[:6]
        for coef_info in top_vars:
            var = coef_info['variable']
            if var in analysis_df.columns:
                scatter_data[var] = [
                    {
                        "x": round(safe_float(x_val), 4),
                        "y": round(safe_float(y_val), 4)
                    }
                    for x_val, y_val in zip(analysis_df[var].values[:100], y_values[:100])  # ìµœëŒ€ 100ê°œ í¬ì¸íŠ¸
                ]
        
        # =====================================================
        # 12ë‹¨ê³„: ì”ì°¨ í†µê³„
        # =====================================================
        residual_stats = {
            "mean": round(safe_float(np.mean(residuals)), 6),
            "std": round(safe_float(np.std(residuals)), 4),
            "min": round(safe_float(np.min(residuals)), 4),
            "max": round(safe_float(np.max(residuals)), 4),
            "mae": round(mae, 4),
            "rmse": round(rmse, 4)
        }
        
        # =====================================================
        # ê²°ê³¼ í•´ì„ ìƒì„± (CV RÂ² ê¸°ë°˜)
        # =====================================================
        interpretation = generate_lasso_interpretation(
            optimal_alpha, cv_mean, mae, rmse,  # r_squared ëŒ€ì‹  cv_mean ì‚¬ìš©
            selected_vars, removed_by_correlation, zero_vars,
            original_scale_coefficients, cv_mean, cv_std, r_squared_train
        )
        
        # =====================================================
        # ê²°ê³¼ ë°˜í™˜
        # =====================================================
        result = {
            "success": True,
            "method": "lasso",
            "n_observations": int(initial_count),
            "dependent_variable": dependent_var,
            "independent_variables": selected_vars,
            "final_main_vars": selected_vars,
            "interaction_terms": [],  # LassoëŠ” ìƒí˜¸ì‘ìš© í•­ ë¯¸ìƒì„±
            "removed_vars": removed_by_correlation + [f"{v} (Lasso ì œì™¸)" for v in zero_vars],
            
            # íšŒê·€ì‹
            "regression_equation": regression_equation,
            
            # Lasso ì „ìš© ê²°ê³¼
            "lasso_results": {
                "optimal_alpha": round(optimal_alpha, 6),
                "correlation_threshold": correlation_threshold,
                "filtered_vars_by_correlation": filtered_vars,
                "correlations_with_y": correlations_with_y,
                "selected_vars": selected_vars,
                "zero_coefficient_vars": zero_vars,
                "coefficients": original_scale_coefficients,
                "intercept": round(float(intercept), 6),
                "cv_r2_mean": round(cv_mean, 4),
                "cv_r2_std": round(cv_std, 4)
            },
            
            # ëª¨ë¸ ìš”ì•½ (LassoëŠ” CV RÂ²ê°€ ì£¼ìš” ì§€í‘œ)
            "model_summary": {
                "r": round(safe_float(np.sqrt(max(0, cv_mean))), 4),  # CV RÂ² ê¸°ë°˜
                "r_squared": round(safe_float(cv_mean), 4),  # CV RÂ² (ì§„ì§œ ì˜ˆì¸¡ ì„±ëŠ¥)
                "adj_r_squared": round(safe_float(r_squared), 4),  # êµì°¨ê²€ì¦ ì˜ˆì¸¡ ê¸°ë°˜ RÂ²
                "std_error_estimate": round(rmse, 4),
                "mae": round(mae, 4),
                "rmse": round(rmse, 4),
                "cv_r2_mean": round(cv_mean, 4),
                "cv_r2_std": round(cv_std, 4),
                "r_squared_train": round(safe_float(r_squared_train), 4),  # í›ˆë ¨ RÂ² (ì°¸ê³ ìš©)
                "f_statistic": None,  # LassoëŠ” F-í†µê³„ëŸ‰ ë¯¸ì œê³µ
                "f_pvalue": None,
                "aic": None,
                "bic": None,
                "log_likelihood": None,
                "durbin_watson": None
            },
            
            # ANOVA (LassoëŠ” ì œê³µí•˜ì§€ ì•ŠìŒ)
            "anova_table": [],
            
            # ê³„ìˆ˜ (Lasso ìŠ¤íƒ€ì¼)
            "coefficients": [
                {
                    "variable": "ìƒìˆ˜í•­",
                    "b": round(float(intercept), 6),
                    "std_error": None,
                    "beta": None,
                    "t_statistic": None,
                    "p_value": None,
                    "tolerance": None,
                    "vif": None,
                    "var_type": "constant"
                }
            ] + [
                {
                    "variable": c["variable"],
                    "b": c["coefficient"],
                    "std_error": None,
                    "beta": c["coefficient_scaled"],  # í‘œì¤€í™” ê³„ìˆ˜
                    "t_statistic": None,
                    "p_value": None,
                    "tolerance": None,
                    "vif": None,
                    "var_type": "main"
                }
                for c in original_scale_coefficients
            ],
            
            # ê¸°ìˆ í†µê³„ëŸ‰
            "descriptive_stats": descriptive_stats,
            
            # ìƒê´€í–‰ë ¬
            "correlation_matrix": correlation_matrix,
            
            # ì”ì°¨ í†µê³„
            "residual_stats": residual_stats,
            
            # ì‹¤ì¸¡ì¹˜/ì˜ˆì¸¡ì¹˜
            "actual_vs_predicted": actual_vs_predicted,
            
            # ì‚°ì ë„ ë°ì´í„°
            "scatter_data": scatter_data,
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì •ë³´
            "missing_value_info": missing_info,
            
            # í•´ì„
            "interpretation": interpretation
        }
        
        return result
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"error": f"Lasso ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}


def generate_lasso_interpretation(optimal_alpha: float, cv_r2: float,
                                   mae: float, rmse: float,
                                   selected_vars: List[str],
                                   removed_by_correlation: List[str],
                                   zero_vars: List[str],
                                   coefficients: List[Dict],
                                   cv_mean: float, cv_std: float,
                                   r_squared_train: float = None) -> str:
    """
    Lasso íšŒê·€ë¶„ì„ ê²°ê³¼ í•´ì„ ìƒì„± (CV RÂ² ê¸°ë°˜)
    """
    interpretation = []
    
    # 1. ëª¨ë¸ ì„¤ëª…ë ¥ (CV RÂ² ê¸°ì¤€)
    if cv_r2 >= 0.7:
        interpretation.append(f"Lasso ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤ (CV RÂ² = {cv_r2:.3f}).")
    elif cv_r2 >= 0.5:
        interpretation.append(f"Lasso ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤ (CV RÂ² = {cv_r2:.3f}).")
    elif cv_r2 >= 0.3:
        interpretation.append(f"Lasso ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë³´í†µì…ë‹ˆë‹¤ (CV RÂ² = {cv_r2:.3f}).")
    else:
        interpretation.append(f"Lasso ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤ (CV RÂ² = {cv_r2:.3f}). ì¶”ê°€ ë³€ìˆ˜ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # 2. ê³¼ì í•© ê²½ê³ 
    if r_squared_train and r_squared_train > 0.9 and cv_r2 < 0.5:
        interpretation.append(f"âš ï¸ ê³¼ì í•© ì£¼ì˜: í›ˆë ¨ RÂ²({r_squared_train:.2f})ì™€ CV RÂ²({cv_r2:.2f})ì˜ ì°¨ì´ê°€ í½ë‹ˆë‹¤.")
    
    # 3. ìµœì  alpha
    interpretation.append(f"ìµœì  ì •ê·œí™” ê°•ë„: Î± = {optimal_alpha:.6f}.")
    
    # 4. ë³€ìˆ˜ ì„ íƒ ê²°ê³¼
    if len(selected_vars) <= 5:
        interpretation.append(f"Lassoê°€ {len(selected_vars)}ê°œ ë³€ìˆ˜ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤: {', '.join(selected_vars)}.")
    else:
        interpretation.append(f"Lassoê°€ {len(selected_vars)}ê°œ ë³€ìˆ˜ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
    
    # 5. ì œê±°ëœ ë³€ìˆ˜
    total_removed = len(removed_by_correlation) + len(zero_vars)
    if total_removed > 0:
        interpretation.append(f"ì´ {total_removed}ê°œ ë³€ìˆ˜ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤ (ìƒê´€ê³„ìˆ˜ í•„í„°ë§: {len(removed_by_correlation)}ê°œ, Lasso ì œì™¸: {len(zero_vars)}ê°œ).")
    
    # 6. ì˜ˆì¸¡ ì •í™•ë„
    interpretation.append(f"ì˜ˆì¸¡ ì˜¤ì°¨: MAE = {mae:.4f}, RMSE = {rmse:.4f}.")
    
    # 7. êµì°¨ê²€ì¦ ì‹ ë¢°êµ¬ê°„
    interpretation.append(f"êµì°¨ê²€ì¦ RÂ²: {cv_mean:.4f} (Â±{cv_std:.4f}).")
    
    # 8. ì£¼ìš” ì˜í–¥ ë³€ìˆ˜
    if coefficients:
        sorted_coefs = sorted(coefficients, key=lambda x: abs(x['coefficient_scaled']), reverse=True)
        top_vars = [c['variable'] for c in sorted_coefs[:3]]
        interpretation.append(f"ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” ë³€ìˆ˜: {', '.join(top_vars)}.")
    
    return ' '.join(interpretation)
